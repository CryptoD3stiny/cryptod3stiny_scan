#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Web Vulnerability Scanner (Defensive)
Author: cryptod3stiny
Usage: Authorized security testing only
"""

__author__ = "cryptod3stiny"
__version__ = "1.0.0"
__license__ = "MIT"

import requests
import argparse
from urllib.parse import urljoin, urlparse, parse_qs
from bs4 import BeautifulSoup
from time import sleep

# ===================== CONFIG =====================
TIMEOUT = 10
HEADERS = {
    "User-Agent": "cryptod3stiny-scanner/1.0"
}

SQL_ERRORS = [
    "you have an error in your sql syntax",
    "warning: mysql",
    "unclosed quotation mark",
    "quoted string not properly terminated",
    "pdoexception",
    "sqlstate"
]

# ===================== UI =====================
def banner():
    print(r"""
 ██████╗██████╗ ██╗   ██╗██████╗ ████████╗ ██████╗ 
██╔════╝██╔══██╗╚██╗ ██╔╝██╔══██╗╚══██╔══╝██╔═══██╗
██║     ██████╔╝ ╚████╔╝ ██████╔╝   ██║   ██║   ██║
██║     ██╔══██╗  ╚██╔╝  ██╔═══╝    ██║   ██║   ██║
╚██████╗██║  ██║   ██║   ██║        ██║   ╚██████╔╝
 ╚═════╝╚═╝  ╚═╝   ╚═╝   ╚═╝        ╚═╝    ╚═════╝

 Web Vulnerability Scanner (Defensive)
 by cryptod3stiny
---------------------------------------------------
 Authorized testing only. Educational use.
    """)

# ===================== CRAWLER =====================
def crawl(url, depth, visited):
    if depth == 0 or url in visited:
        return set()

    print(f"[+] Crawling: {url}")
    visited.add(url)
    found = set()

    try:
        r = requests.get(url, headers=HEADERS, timeout=TIMEOUT)
        soup = BeautifulSoup(r.text, "html.parser")

        for link in soup.find_all("a", href=True):
            href = urljoin(url, link['href'])
            parsed = urlparse(href)

            if parsed.scheme.startswith("http") and parsed.netloc == urlparse(url).netloc:
                found.add(href)
                found |= crawl(href, depth - 1, visited)

    except Exception:
        pass

    return found

# ===================== ANALYSIS =====================
def analyze_xss(url):
    issues = []
    parsed = urlparse(url)
    params = parse_qs(parsed.query)

    for p in params:
        issues.append({
            "type": "XSS",
            "endpoint": url,
            "detail": f"Parametro reflejable detectado: {p}",
            "risk": "Medio"
        })

    return issues

def analyze_sqli(url):
    issues = []
    try:
        r = requests.get(url, headers=HEADERS, timeout=TIMEOUT)
        content = r.text.lower()

        for err in SQL_ERRORS:
            if err in content:
                issues.append({
                    "type": "SQLi",
                    "endpoint": url,
                    "detail": "Posible error SQL detectado en la respuesta",
                    "risk": "Alto"
                })
                break

    except Exception:
        pass

    return issues

def analyze_idor(url):
    issues = []
    parsed = urlparse(url)
    params = parse_qs(parsed.query)

    for p in params:
        if p.lower() in ["id", "user", "account", "profile", "uid"]:
            issues.append({
                "type": "IDOR",
                "endpoint": url,
                "detail": f"Parametro sensible detectado: {p}",
                "risk": "Medio"
            })

    return issues

# ===================== REPORT =====================
def print_report(results):
    if not results:
        print("\n[+] No se detectaron vulnerabilidades aparentes.")
        return

    print("\n[!] Posibles vulnerabilidades detectadas:\n")

    for r in results:
        print(f"Tipo      : {r['type']}")
        print(f"Endpoint  : {r['endpoint']}")
        print(f"Detalle   : {r['detail']}")
        print(f"Riesgo    : {r['risk']}")
        print("-" * 50)

# ===================== MAIN =====================
def main():
    parser = argparse.ArgumentParser(
        description="Web Vulnerability Scanner - cryptod3stiny"
    )

    parser.add_argument("-u", "--url", required=True, help="URL objetivo")
    parser.add_argument("--depth", type=int, default=2, help="Profundidad del crawler")
    parser.add_argument("--xss", action="store_true", help="Analizar XSS")
    parser.add_argument("--sqli", action="store_true", help="Analizar SQLi")
    parser.add_argument("--idor", action="store_true", help="Analizar IDOR")
    parser.add_argument("--all", action="store_true", help="Analizar todo")

    args = parser.parse_args()

    banner()

    target = args.url.rstrip("/")
    visited = set()

    print("[+] Iniciando rastreo...")
    endpoints = crawl(target, args.depth, visited)
    endpoints.add(target)

    print(f"\n[+] Endpoints descubiertos: {len(endpoints)}")

    results = []

    for ep in endpoints:
        if args.all or args.xss:
            results.extend(analyze_xss(ep))
        if args.all or args.sqli:
            results.extend(analyze_sqli(ep))
        if args.all or args.idor:
            results.extend(analyze_idor(ep))

        sleep(0.2)

    print_report(results)

    print("\nEscaneo completado ✔")
    print("by cryptod3stiny")

if __name__ == "__main__":
    main()
